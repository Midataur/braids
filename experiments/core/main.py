CONFIG = {
    "modelname": "test-1",
    "model_type": "RegressionModel",
    "dataset": "initialtests",
    "dataset_type": "basicregression",

    # dataset parameters
    "braid_count": 3,
    "max_word_length": 6,

    # number of tokens in the vocabulary
    # it's braid_count * 2 + 1
    # TODO: make this derived
    "vocab_size": 7,

    # length of the input
    # it's the same as max word length
    # TODO: make this derived
    "context_length": 6,

    # embedding dimension
    # good starting value: 402
    "n_embed": 180,

    # number of attention heads:
    # good starting value: 6
    # n_embed % n_heads must be 0
    "n_heads": 6,

    # dropout factor to use
    # i usually set to zero
    "dropout": 0,

    # number of blocks to have
    # higher means a deeper network
    "n_blocks": 8,

    # number of continuous variables
    # currently 12, may change
    "n_cont": 12,

    # good starting value: 3*10^-4
    "learning_rate": 3*(10**-4), 

    # good starting value: 64
    "batchsize": 512, 

    # good starting value: 0.1
    "weight_decay": 0.0001, 

    # usually 0.1
    "lr_factor": 0.1, 

    # usually 10
    "lr_patience": 10, 

    # usually 0.01
    "threshold": 0.01, 

    # number of workers to use for loading data to the gpus
    # set to 0 for all, +ve for specific
    # usually 0
    "n_workers": 0, 

    # when loading data, what do we do if there are too many categorical variables?
    # if true, display a warning and skip that one
    # if false, throw an error
    # usually want true, since outliers aren't worth an error
    "discard_too_long": True, # set to true unless you want to fight your data

    # should be . unless you're doing something weird
    "PATH": ".",
}

assert CONFIG["n_embed"] % CONFIG["n_heads"] == 0

if __name__ == "__main__":
    print("Loading libraries...")
    from training import train
    train(CONFIG)